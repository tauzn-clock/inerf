{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch import tensor\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation \n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from inerf.inerf_trainer import INerfTrainer\n",
    "from inerf.inerf_utils import get_corrected_pose, load_eval_image_into_pipeline, get_relative_pose, get_absolute_diff_for_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace')\n",
    "MODEL_PATH = \"/workspace/outputs/jackal_training_data_1/plane-nerf/2024-01-14_115715\"\n",
    "EVAL_PATH = \"/stored_data/jackal_evaluation_data\"\n",
    "TRANSFORM_PATH = os.path.join(EVAL_PATH, \"transforms.json\")\n",
    "with open(TRANSFORM_PATH) as f:\n",
    "    TRANSFORM_JSON = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">WARNING: Using slower TCNN CutlassMLP instead of TCNN FullyFusedMLP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWARNING: Using slower TCNN CutlassMLP instead of TCNN FullyFusedMLP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Use layer width of </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">16</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">32</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">64</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, or </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">128</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> to use the faster TCNN FullyFusedMLP.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mUse layer width of \u001b[0m\u001b[1;33m16\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m32\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m64\u001b[0m\u001b[1;33m, or \u001b[0m\u001b[1;33m128\u001b[0m\u001b[1;33m to use the faster TCNN FullyFusedMLP.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">WARNING: Using slower TCNN CutlassMLP instead of TCNN FullyFusedMLP</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWARNING: Using slower TCNN CutlassMLP instead of TCNN FullyFusedMLP\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Use layer width of </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">16</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">32</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">64</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">, or </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">128</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> to use the faster TCNN FullyFusedMLP.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mUse layer width of \u001b[0m\u001b[1;33m16\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m32\u001b[0m\u001b[1;33m, \u001b[0m\u001b[1;33m64\u001b[0m\u001b[1;33m, or \u001b[0m\u001b[1;33m128\u001b[0m\u001b[1;33m to use the faster TCNN FullyFusedMLP.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "outputs/jackal_training_data_1/plane-nerf/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-14_115715/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000009999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "outputs/jackal_training_data_1/plane-nerf/\u001b[1;36m2024\u001b[0m-\u001b[1;36m01\u001b[0m-14_115715/nerfstudio_models/step-\u001b[1;36m000009999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_path = os.path.join(MODEL_PATH, \"config.yml\")\n",
    "config, pipeline, checkpoint_path, _ = eval_setup(\n",
    "                        Path(config_path),\n",
    "                        test_mode=\"inference\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 2.5\n",
    "pitch = 0.785\n",
    "r = -2.5\n",
    "init_tf = np.eye(4)\n",
    "init_tf[:3, :3] = Rotation.from_rotvec(np.array([pitch, 0, 0])).as_matrix()\n",
    "init_tf[0, 3] = -r\n",
    "init_tf[2, 3] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/inerf/inerf/inerf_utils.py:100: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  camera_to_worlds = torch.cat([camera_to_worlds, tensor([tf]).float()], 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m300\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c5a359b3184589b408f26b3aad6eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest Nerfstudio checkpoint from load_dir...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Done loading Nerfstudio checkpoint from \n",
       "outputs/jackal_training_data_1/plane-nerf/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">01</span>-14_115715/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000009999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Done loading Nerfstudio checkpoint from \n",
       "outputs/jackal_training_data_1/plane-nerf/\u001b[1;36m2024\u001b[0m-\u001b[1;36m01\u001b[0m-14_115715/nerfstudio_models/step-\u001b[1;36m000009999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = load_eval_image_into_pipeline(pipeline,EVAL_PATH,starting_pose=init_tf)\n",
    "custom_camera_optimizer = deepcopy(pipeline.model.camera_optimizer)\n",
    "custom_camera_optimizer.num_cameras = len(pipeline.datamanager.train_dataset.cameras)\n",
    "trainer = INerfTrainer(config)\n",
    "trainer.pipeline = pipeline\n",
    "trainer.pipeline.model.camera_optimizer = custom_camera_optimizer\n",
    "trainer.setup(\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_poses = []\n",
    "for _ , batch in pipeline.datamanager.fixed_indices_train_dataloader:\n",
    "    tf = TRANSFORM_JSON[\"frames\"][int(batch['image_idx'])][\"transform_matrix\"]\n",
    "    tf = np.asarray(tf)\n",
    "    tf = tf[:3, :4 ]\n",
    "    ground_truth_poses.append(tf)\n",
    "ground_truth_poses = torch.tensor(ground_truth_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation:  tensor([ 2.5005, -0.0086,  2.4977])\n",
      "Rotation:  [ 4.52073562e+01  2.35932408e-02 -1.64198591e-01]\n",
      "Translation:  tensor([ 2.4997, -0.0085,  2.4957])\n",
      "Rotation:  [45.19307456  0.06547239 -0.13200478]\n",
      "Translation:  tensor([ 2.5030, -0.0128,  2.5000])\n",
      "Rotation:  [ 4.51844090e+01 -1.50405631e-02 -2.09000180e-01]\n",
      "Translation:  tensor([ 2.5039, -0.0166,  2.5020])\n",
      "Rotation:  [ 4.51641413e+01 -3.22469756e-02 -2.28376283e-01]\n",
      "Translation:  tensor([ 2.5058, -0.0198,  2.5042])\n",
      "Rotation:  [45.14832546 -0.06738836 -0.26416467]\n",
      "Translation:  tensor([ 2.5074, -0.0229,  2.5065])\n",
      "Rotation:  [45.12327166 -0.10039196 -0.29803314]\n",
      "Translation:  tensor([ 2.5099, -0.0251,  2.5095])\n",
      "Rotation:  [45.10634841 -0.16214491 -0.35728928]\n",
      "Translation:  tensor([ 2.5129, -0.0285,  2.5123])\n",
      "Rotation:  [45.08660404 -0.23007174 -0.42422377]\n",
      "Translation:  tensor([ 2.5136, -0.0295,  2.5111])\n",
      "Rotation:  [45.0609586  -0.22959402 -0.43073676]\n",
      "Translation:  tensor([ 2.5148, -0.0309,  2.5107])\n",
      "Rotation:  [45.0245333  -0.25377417 -0.46090817]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for i in range(10):\n",
    "    for j in range(n):\n",
    "        trainer.train_iteration_inerf(i*n + j)\n",
    "    corrected_pose = get_corrected_pose(trainer)\n",
    "    R = corrected_pose.cpu().detach()[0,:3,:3]\n",
    "    t = corrected_pose.cpu().detach()[0,:3,3]\n",
    "    rpy = Rotation.from_matrix(R).as_euler('xyz', degrees=True)\n",
    "    print(\"Translation: \", t)\n",
    "    print(\"Rotation: \", rpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average translation error:  tensor(3.5597, dtype=torch.float64)\n",
      "Average rotation error:  tensor(1.5957, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "relative_pose = get_relative_pose(ground_truth_poses, corrected_pose)\n",
    "t_diff, r_diff = get_absolute_diff_for_pose(relative_pose)\n",
    "\n",
    "#Get averrage absolute translation and rotation error\n",
    "print(\"Average translation error: \", torch.mean(t_diff))\n",
    "print(\"Average rotation error: \", torch.mean(r_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
