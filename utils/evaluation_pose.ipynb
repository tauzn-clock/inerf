{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch import tensor\n",
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.transform import Rotation \n",
    "from nerfstudio.cameras.camera_optimizers import CameraOptimizer\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from inerf.inerf_trainer import INerfTrainer\n",
    "from inerf.inerf_utils import get_corrected_pose, load_eval_image_into_pipeline, get_relative_pose, get_absolute_diff_for_pose, get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace')\n",
    "MODEL_PATH = \"/workspace/outputs/jackal_training_data_1/plane-nerf/2024-01-14_115715\"\n",
    "EVAL_PATH = \"/stored_data/jackal_one_frame\"\n",
    "GROUND_TRUTH_PATH = os.path.join(EVAL_PATH, \"ground_truth.json\")\n",
    "TRANSFORM_FILE = \"transforms.json\"\n",
    "with open(GROUND_TRUTH_PATH) as f:\n",
    "    GROUND_TRUTH = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(MODEL_PATH, \"config.yml\")\n",
    "config, pipeline, checkpoint_path, _ = eval_setup(\n",
    "                        Path(config_path),\n",
    "                        test_mode=\"inference\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 2.5\n",
    "pitch = 0.785\n",
    "r = -2.5\n",
    "init_tf = np.eye(4)\n",
    "init_tf[:3, :3] = Rotation.from_rotvec(np.array([pitch, 0, 0])).as_matrix()\n",
    "init_tf[0, 3] = -r\n",
    "init_tf[2, 3] = z\n",
    "init_tf = [init_tf]*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load_eval_image_into_pipeline(pipeline,EVAL_PATH,transform_file=TRANSFORM_FILE)\n",
    "from plane_nerf.plane_nerf_optimizer import PlaneNerfCameraOptimizer\n",
    "\n",
    "#custom_camera_optimizer = CameraOptimizer(\n",
    "custom_camera_optimizer = PlaneNerfCameraOptimizer(\n",
    "    config = pipeline.model.camera_optimizer.config,\n",
    "    num_cameras = len(pipeline.datamanager.train_dataset),\n",
    "    device = pipeline.device,\n",
    ")\n",
    "custom_camera_optimizer.config.rot_l2_penalty = 0 #\n",
    "custom_camera_optimizer.config.trans_l2_penalty = 0 #\n",
    "pipeline.model.camera_optimizer = custom_camera_optimizer\n",
    "trainer = INerfTrainer(config)\n",
    "trainer.setup_inerf(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_poses = []\n",
    "for _ , batch in pipeline.datamanager.fixed_indices_train_dataloader:\n",
    "    tf = GROUND_TRUTH[\"frames\"][int(batch['image_idx'])][\"transform_matrix\"]\n",
    "    tf = np.asarray(tf)\n",
    "    tf = tf[:3, :4 ]\n",
    "    ground_truth_poses.append(tf)\n",
    "ground_truth_poses = torch.tensor(ground_truth_poses).to(pipeline.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop = 10\n",
    "n = 100\n",
    "lr_max = 5e-3\n",
    "lr_min = 1e-4\n",
    "store = torch.tensor([])\n",
    "\n",
    "corrected_pose = get_corrected_pose(trainer)\n",
    "#print(corrected_pose)\n",
    "relative_pose = get_relative_pose(ground_truth_poses, corrected_pose)\n",
    "t_diff, r_diff = get_absolute_diff_for_pose(relative_pose)\n",
    "#Get averrage absolute translation and rotation error\n",
    "print(\"Average translation error: \", torch.mean(t_diff))\n",
    "print(\"Average rotation error: \", torch.mean(r_diff))\n",
    "\n",
    "store = torch.cat((store, torch.tensor([[0, torch.mean(t_diff), torch.mean(r_diff)]])), 0)\n",
    "\n",
    "for i in range(train_loop):\n",
    "    for j in range(n):\n",
    "        lr = lr_min + (lr_max - lr_min) * (i / train_loop)\n",
    "        trainer.pipeline.train()\n",
    "        loss, loss_dict, metrics_dict = trainer.train_iteration_inerf(i*n + j,optimizer_lr = 1e-4)\n",
    "    corrected_pose = get_corrected_pose(trainer)\n",
    "    #print(corrected_pose)\n",
    "    relative_pose = get_relative_pose(ground_truth_poses, corrected_pose)\n",
    "    t_diff, r_diff = get_absolute_diff_for_pose(relative_pose)\n",
    "\n",
    "    #Get averrage absolute translation and rotation error\n",
    "    print(\"Average translation error: \", torch.mean(t_diff))\n",
    "    print(\"Average rotation error: \", torch.mean(r_diff))\n",
    "    \n",
    "    store = torch.cat((store, torch.tensor([[i+1, torch.mean(t_diff), torch.mean(r_diff)]])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = cv2.imread(os.path.join(EVAL_PATH, GROUND_TRUTH[\"frames\"][0][\"file_path\"]))\n",
    "original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "rendered_img = get_image(trainer.pipeline, corrected_pose[0:,:,:])\n",
    "rendered_img = rendered_img[\"rgb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot original image and rendered image\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(original_img)\n",
    "ax[0].set_title(\"Original image\")\n",
    "ax[1].imshow(rendered_img)\n",
    "ax[1].set_title(\"Rendered image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overlay original image and rendered image\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.imshow(original_img)\n",
    "ax.imshow(rendered_img, alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot store_t_diff and store_r_diff with  respect to training iteration in 2 subplots\n",
    "\n",
    "plotting_data = np.asarray(store.to(\"cpu\"))\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(plotting_data[:, 0], plotting_data[:, 1])\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Absolute translation error\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(plotting_data[:, 0], plotting_data[:, 2])\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Absolute rotation error\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
