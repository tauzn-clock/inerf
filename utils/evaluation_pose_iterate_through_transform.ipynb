{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation \n",
    "from IPython.display import clear_output\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from inerf.inerf_trainer import load_data_into_trainer\n",
    "from inerf.inerf_utils import get_corrected_pose, load_eval_image_into_pipeline, get_relative_pose, get_absolute_diff_for_pose, get_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/workspace')\n",
    "MODEL_PATH = \"/workspace/outputs/jackal_with_box/plane-nerf-simplified/2024-02-13_163941\"\n",
    "DATA_PATH = \"/stored_data/jackal_with_box_eval/\"\n",
    "GROUND_TRUTH_PATH = os.path.join(DATA_PATH, \"ground_truth.json\")\n",
    "TRANSFORM_FILE = \"transforms_10.json\"\n",
    "with open(GROUND_TRUTH_PATH) as f:\n",
    "    GROUND_TRUTH = json.load(f)\n",
    "with open(os.path.join(DATA_PATH, TRANSFORM_FILE)) as f:\n",
    "    TRANSFORM = json.load(f)\n",
    "SAVE_FILE_NAME = \"eval_results_\"+str(time.strftime(\"%Y-%m-%d_%H%M%S\"))+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "Warning: FullyFusedMLP is not supported for the selected architecture 37. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "outputs/jackal_with_box/plane-nerf-simplified/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-13_163941/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000009999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "outputs/jackal_with_box/plane-nerf-simplified/\u001b[1;36m2024\u001b[0m-\u001b[1;36m02\u001b[0m-13_163941/nerfstudio_models/step-\u001b[1;36m000009999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_path = os.path.join(MODEL_PATH, \"config.yml\")\n",
    "config, pipeline, _, _ = eval_setup(\n",
    "                        Path(config_path),\n",
    "                        test_mode=\"inference\",\n",
    "                        )\n",
    "config.pipeline.datamanager.pixel_sampler.num_rays_per_batch = 4096 \n",
    "\n",
    "train_loop = 10\n",
    "n = 100\n",
    "lr_max = 5e-3\n",
    "lr_min = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Number of keypoints:  40\n",
      "Number of rays:  1981\n",
      "Randomly select more rays\n",
      "Final number of rays:  8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([8192, 3])) that is different to the input size (torch.Size([8192, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     lr \u001b[38;5;241m=\u001b[39m lr_min \u001b[38;5;241m+\u001b[39m (lr_max \u001b[38;5;241m-\u001b[39m lr_min) \u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m/\u001b[39m train_loop)\n\u001b[1;32m     42\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iteration_inerf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m corrected_pose \u001b[38;5;241m=\u001b[39m get_corrected_pose(trainer)\n\u001b[1;32m     46\u001b[0m relative_pose \u001b[38;5;241m=\u001b[39m get_relative_pose(ground_truth_poses, corrected_pose)\n",
      "File \u001b[0;32m~/nerfstudio/nerfstudio/utils/profiler.py:127\u001b[0m, in \u001b[0;36m_TimeFunction.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_call_args \u001b[38;5;241m=\u001b[39m (args, kwargs)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_call_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/workspace/inerf/inerf/inerf_trainer.py:192\u001b[0m, in \u001b[0;36mINerfTrainer.train_iteration_inerf\u001b[0;34m(self, step, optimizer_lr)\u001b[0m\n\u001b[1;32m    188\u001b[0m cpu_or_cuda_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cpu_or_cuda_str \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cpu_or_cuda_str\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39mcpu_or_cuda_str, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixed_precision):\n\u001b[0;32m--> 192\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scaler\u001b[38;5;241m.\u001b[39mscale(loss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    196\u001b[0m needs_step \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamera_opt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m#Updates only the camera optimizer\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/inerf/inerf/inerf_trainer.py:143\u001b[0m, in \u001b[0;36mINerfTrainer.get_loss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m gt_rgb \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# RGB or RGBA image\u001b[39;00m\n\u001b[1;32m    141\u001b[0m predicted_rgb \u001b[38;5;241m=\u001b[39m model_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 143\u001b[0m rgb_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL1Loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# #Pixel Loss\u001b[39;00m\n\u001b[1;32m    146\u001b[0m pixel_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:101\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3263\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3261\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3263\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "\n",
    "for f in range(len(TRANSFORM[\"frames\"])):\n",
    "    transform = TRANSFORM.copy()\n",
    "    transform[\"frames\"] = [TRANSFORM[\"frames\"][f]]\n",
    "    \n",
    "    tf = GROUND_TRUTH[\"frames\"][f][\"transform_matrix\"]\n",
    "    tf = np.asarray(tf)\n",
    "    tf = tf[:3, :4 ]\n",
    "    ground_truth_poses = [tf]\n",
    "    ground_truth_poses = torch.tensor(ground_truth_poses).to(pipeline.device)\n",
    "    \n",
    "    pipeline = load_eval_image_into_pipeline(pipeline,DATA_PATH,transform)\n",
    "\n",
    "    config.pipeline.datamanager.pixel_sampler.num_rays_per_batch = 4096 \n",
    "\n",
    "    trainer = load_data_into_trainer(\n",
    "        config,\n",
    "        pipeline,\n",
    "        plane_optimizer = False\n",
    "    )\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f)\n",
    "    \n",
    "    trainer.pipeline.datamanager.KERNEL_SIZE = 5\n",
    "    trainer.pipeline.datamanager.THRESHOLD = 5\n",
    "    trainer.pipeline.datamanager.METHOD = \"sift\"\n",
    "    trainer.pipeline.datamanager.get_inerf_batch()  \n",
    "        \n",
    "    store = torch.tensor([])\n",
    "\n",
    "    corrected_pose = get_corrected_pose(trainer)\n",
    "\n",
    "    relative_pose = get_relative_pose(ground_truth_poses, corrected_pose)\n",
    "    t_diff, r_diff = get_absolute_diff_for_pose(relative_pose)\n",
    "\n",
    "    store = torch.cat((store, torch.tensor([torch.mean(t_diff), torch.mean(r_diff)])), 0)\n",
    "\n",
    "    for i in range(train_loop):\n",
    "        for j in range(n):\n",
    "            lr = lr_min + (lr_max - lr_min) * (i / train_loop)\n",
    "            trainer.pipeline.train()\n",
    "            loss = trainer.train_iteration_inerf(i*n + j,optimizer_lr = 1e-3)\n",
    "        corrected_pose = get_corrected_pose(trainer)\n",
    "\n",
    "        relative_pose = get_relative_pose(ground_truth_poses, corrected_pose)\n",
    "        t_diff, r_diff = get_absolute_diff_for_pose(relative_pose)\n",
    "\n",
    "        store = torch.cat((store, torch.tensor([torch.mean(t_diff), torch.mean(r_diff)])), 0)\n",
    "    \n",
    "    output.append(store.to(\"cpu\").numpy().tolist())\n",
    "    np.savetxt(os.path.join(MODEL_PATH,SAVE_FILE_NAME), np.asarray(output), delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
